{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "E-Commerce Reviews Analysis with Machine Learning\n",
    "===\n",
    "By Sangeetha Priya RE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "from tkinter import filedialog as fd\n",
    "from tkinter.messagebox import showinfo\n",
    "window = tk.Tk()\n",
    "window.title('E-Commerce Review Processing By pairwise ranking and sentiment analysis')\n",
    "Name = tk.Label(text=\"Select The DataSet\")\n",
    "Name1 = tk.Label(text=\"E-Commerce Review Processing By pairwise ranking and sentiment analysis\")\n",
    "Name2 = tk.Label(text=\"By Kheshore JR\")\n",
    "Name1.pack()\n",
    "Name2.pack()\n",
    "Name.pack()\n",
    "\n",
    "def select_file():\n",
    "    global filename\n",
    "    filename =  0\n",
    "    filetypes = (\n",
    "        ('csv files', '*.csv'),\n",
    "    )\n",
    "\n",
    "    filename = fd.askopenfilename(\n",
    "        title='Select a CSV File',\n",
    "        initialdir='/',\n",
    "        filetypes=filetypes)\n",
    "\n",
    "    showinfo(\n",
    "        title='Selected File',\n",
    "        message=filename\n",
    "    )\n",
    "    showinfo(\n",
    "        title='Processing...',\n",
    "        message='Close the \"Select The DataSet\" Dialog Box to Get Analytics'\n",
    "\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "open_button = ttk.Button(\n",
    "    window,\n",
    "    text='Open a File',\n",
    "    command=select_file\n",
    ")\n",
    "\n",
    "open_button.pack(expand=True)\n",
    "\n",
    "\n",
    "window.mainloop()\n",
    "\n",
    "while (filename!=0):\n",
    "\n",
    "    df = pd.read_csv(filename)\n",
    "    for column in [\"Division Name\",\"Department Name\",\"Class Name\",\"Review Text\"]:\n",
    "        df = df[df[column].notnull()]\n",
    "    df.drop(df.columns[0], inplace=True, axis=1)\n",
    "    df['Label'] = 0\n",
    "    df.loc[df.Rating >= 3, ['Label']] = 1\n",
    "    df['Word Count'] = df['Review Text'].str.split().apply(len)\n",
    "    df.describe().T.drop('count', axis=1)\n",
    "    df[['Title', 'Division Name', 'Department Name', 'Class Name']].describe(include=['O']).T.drop('count', axis=1)\n",
    "    \n",
    "    ## Average Rating and Recommended IND by Class Name Correlation\n",
    "    key = 'Class Name'\n",
    "    temp = (df.groupby(key)[['Rating', 'Recommended IND', 'Age']]\n",
    "            .aggregate(['count', 'mean']))\n",
    "    temp.columns = ['Count', 'Rating Mean', 'Recommended Likelihood Count',\n",
    "                    'Recommended Likelihood', 'Age Count', 'Age Mean']\n",
    "    temp.drop(['Recommended Likelihood Count', 'Age Count'], axis=1, inplace=True)\n",
    "\n",
    "    # Plot Correlation Matrix\n",
    "    f, ax = plt.subplots(figsize=[10, 7])\n",
    "    ax = sns.heatmap(temp.corr(),\n",
    "                    annot=True, fmt='.2f', cbar_kws={'label': 'Correlation Coefficient'})\n",
    "    ax.set_title('Correlation Coefficient for Mean and Count for\\nRating, Recommended Likelihood, and Age\\nGrouped by {}'.format(key))\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('meanrating-recommended-classname-corr.png', format='png', dpi=300)\n",
    "    plt.show()\n",
    "    print('Class Categories:\\n',df['Class Name'].unique())\n",
    "    g = sns.jointplot(y='Recommended Likelihood', x='Age Mean', data=temp,\n",
    "                    kind='reg', color='b')\n",
    "    plt.subplots_adjust(top=0.999)\n",
    "    g.fig.suptitle('Age Mean and Recommended Likelihood\\nGrouped by Clothing Class')\n",
    "    plt.savefig('meanage-recommended-clothing.png', format='png', dpi=300)\n",
    "    plt.ylim(.7, 1.01)\n",
    "    # Working with Text\n",
    "    pd.set_option('max_colwidth', 500)\n",
    "    df[[\"Title\",\"Review Text\", \"Rating\"]].sample(7)\n",
    "    ## Text Cleaning\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def preprocessing(data):\n",
    "        txt = data.str.lower().str.cat(sep=' ') #1\n",
    "        words = tokenizer.tokenize(txt) #2\n",
    "        words = [w for w in words if not w in stop_words] #3\n",
    "        #words = [ps.stem(w) for w in words] #4\n",
    "        return words\n",
    "    import matplotlib as mpl\n",
    "\n",
    "    stopwords = set(STOPWORDS)\n",
    "    size = (20, 10)\n",
    "\n",
    "    def cloud(text, title, stopwords=stopwords, size=size):\n",
    "        mpl.rcParams['figure.figsize'] = (10.0, 10.0)\n",
    "        mpl.rcParams['font.size'] = 12\n",
    "        mpl.rcParams['savefig.dpi'] = 300\n",
    "        mpl.rcParams['figure.subplot.bottom'] = .1\n",
    "        \n",
    "        wordcloud = WordCloud(width=1600, height=800,\n",
    "                            background_color='black',\n",
    "                            stopwords=stopwords).generate(str(text))\n",
    "        \n",
    "        fig = plt.figure(figsize=size, facecolor='k')\n",
    "        plt.imshow(wordcloud)\n",
    "        plt.axis('off')\n",
    "        plt.title(title, fontsize=50, color='y')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('{}.png'.format(title), format='png', dpi=300)\n",
    "        plt.show()\n",
    "        \n",
    "    def wordfreqviz(text, x):\n",
    "        word_dist = nltk.FreqDist(text)\n",
    "        top_N = x\n",
    "        rslt = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                            columns=['Word', 'Frequency']).set_index('Word')\n",
    "        mpl.style.use('ggplot')\n",
    "        rslt.plot.bar(rot=0)\n",
    "        \n",
    "    def wordfreq(text, x):\n",
    "        word_dist = nltk.FreqDist(text)\n",
    "        top_N = x\n",
    "        rlst = pd.DataFrame(word_dist.most_common(top_N),\n",
    "                            columns=['Word', 'Frequency']).set_index('Word')\n",
    "        return rlst\n",
    "    new_stop = set(STOPWORDS)\n",
    "    new_stop.update([x.lower() for x in list(df['Class Name'][df['Class Name'].notnull()].unique())]\n",
    "                    + ['dress', 'petite'])\n",
    "\n",
    "    # Cloud\n",
    "    cloud(text=df.Title[df.Title.notnull()].astype(str).values,\n",
    "        title='WordCloud for Titles',\n",
    "        stopwords=new_stop,\n",
    "        size = (7,4))\n",
    "    title ='Most Frequent Words in Highly Rated Comments'\n",
    "    temp = df['Review Text'][df.Rating.astype(int) >= 3]\n",
    "\n",
    "    # Modify Stopwords to Exclude Class types, suchs as 'dress'\n",
    "    new_stop = set(STOPWORDS)\n",
    "    new_stop.update([x.lower() for x in list(df['Class Name'][df['Class Name'].notnull()].unique())]\n",
    "                    + ['dress', 'petite'])\n",
    "\n",
    "    # Cloud\n",
    "    cloud(text= temp.values, title=title,stopwords= new_stop)\n",
    "\n",
    "    # Bar Chart\n",
    "    wordfreq(preprocessing(temp), 20).plot.bar(rot=45, legend=False, figsize=(15, 5), color='g',\n",
    "                                            title=title)\n",
    "    plt.ylabel('Occurrence Count')\n",
    "    plt.xlabel('Most Frequent Words')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('most-freq-words-high-rate-comments.png', format='png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    # Low Raited\n",
    "    title ='Most Frequent Words in Low Rated Comments'\n",
    "    temp = df['Review Text'][df.Rating.astype(int) < 3]\n",
    "\n",
    "    # Modify Stopwords to Exclude Class types, suchs as 'dress'\n",
    "    new_stop = set(STOPWORDS)\n",
    "    new_stop.update([x.lower() for x in list(df['Class Name'][df['Class Name'].notnull()].unique())]\n",
    "                    + ['dress', 'petite', 'skirt', 'shirt'])\n",
    "\n",
    "    # Cloud\n",
    "    cloud(temp.values, title=title, stopwords=new_stop)\n",
    "    reviews = df['Review Text'].astype(str).str.lower()\n",
    "    type(reviews)\n",
    "    features = reviews.tolist()\n",
    "    features\n",
    "    import re\n",
    "    from string import punctuation\n",
    "    for index in range(len(features)):\n",
    "        all_text = ''.join([character for character in features[index] if character not in punctuation])\n",
    "        features[index] = re.split(r'\\n|\\r', all_text)\n",
    "        features[index] = ' '.join([word for word in features[index]])\n",
    "    features\n",
    "    labels = np.array(df['Recommended IND'], np.int)\n",
    "    labels.shape\n",
    "    labels[labels == 1].shape[0]\n",
    "    labels[labels == 0].shape[0]\n",
    "    from keras.utils import to_categorical\n",
    "    labels = to_categorical(labels)\n",
    "    labels[:10]\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(features)\n",
    "    vocabulary_size = len(t.word_index) + 1\n",
    "    print('Vocabulary size : {}'.format(vocabulary_size))\n",
    "    encoded_features = t.texts_to_sequences(features)\n",
    "\n",
    "    max_length = 300\n",
    "\n",
    "    padded_features = pad_sequences(encoded_features, maxlen=max_length, padding='post')\n",
    "    embeddings_index = dict()\n",
    "    with open('/home/darth/GitHub Projects/senti-internship-notes/abienagarap/word-vectors/glove.840B.300d.txt') as file:\n",
    "        data = file.readlines()\n",
    "        \n",
    "    # store <key, value> pair of FastText vectors\n",
    "    for line in data[1:]:\n",
    "        word, vec = line.split(' ', 1)\n",
    "        embeddings_index[word] = np.array([float(index) for index in vec.split()], dtype='float32')\n",
    "    print('Loaded {} word vectors.'.format(len(embeddings_index)))\n",
    "\n",
    "\n",
    "    embedding_matrix = np.zeros((vocabulary_size, max_length))\n",
    "    for word, i in t.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    words = []\n",
    "    for word, i in t.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            words.append(word)\n",
    "    print('{} words covered.'.format(len(words)))\n",
    "    percentage = (len(words) / vocabulary_size) * 100.00\n",
    "    print('{}% of {} words were covered'.format(percentage, vocabulary_size))\n",
    "    def train_test_split(features, labels, **kwargs):\n",
    "        \n",
    "        # concatenate the features and labels array\n",
    "        dataset = np.c_[features, labels]\n",
    "\n",
    "        # shuffle the dataset\n",
    "        np.random.shuffle(dataset)\n",
    "\n",
    "        # split the dataset into features, labels\n",
    "        features, labels = dataset[:, 0:max_length], dataset[:, max_length:]\n",
    "\n",
    "        # get the split size for training dataset\n",
    "        split_index = int(kwargs['train_size'] * len(features))\n",
    "\n",
    "        # split the dataset into training/validation dataset\n",
    "        train_features, validation_features = features[:split_index], features[split_index:]\n",
    "        train_labels, validation_labels = labels[:split_index], labels[split_index:]\n",
    "\n",
    "        # get the split size for validation dataset\n",
    "        split_index = int(kwargs['validation_size'] * len(validation_features))\n",
    "\n",
    "        # split the validation dataset into validation/testing dataset\n",
    "        validation_features, test_features = validation_features[:split_index], validation_features[split_index:]\n",
    "        validation_labels, test_labels = validation_labels[:split_index], validation_labels[split_index:]\n",
    "\n",
    "        # return the partitioned dataset\n",
    "        return [train_features, train_labels], [validation_features, validation_labels], [test_features, test_labels]\n",
    "    train_dataset, validation_dataset, test_dataset = train_test_split(features=padded_features, labels=labels,\n",
    "                                                                    train_size=0.60, validation_size=0.50)\n",
    "    print('Dataset size : {}'.format(padded_features.shape[0]))\n",
    "    print('Train dataset size : {}'.format(train_dataset[0].shape[0]))\n",
    "    print('Validation dataset size : {}'.format(validation_dataset[0].shape[0]))\n",
    "    print('Test dataset size : {}'.format(test_dataset[0].shape[0]))\n",
    "    from keras import callbacks\n",
    "    from keras.layers import Bidirectional\n",
    "    from keras.layers import Dense\n",
    "    from keras.layers import Dropout\n",
    "    from keras.layers import Embedding\n",
    "    from keras.layers import LSTM\n",
    "    from keras.models import Sequential\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    model = Sequential()\n",
    "    e = Embedding(vocabulary_size, max_length,\n",
    "                weights=[embedding_matrix], input_length=max_length, trainable=False)\n",
    "    model.add(e)\n",
    "    model.add(Bidirectional(LSTM(256)))\n",
    "    model.add(Dropout(0.50))\n",
    "    model.add(Dense(2, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(train_dataset[0], train_dataset[1], epochs=32, batch_size=256, verbose=1,\n",
    "            validation_data=(validation_dataset[0], validation_dataset[1]))\n",
    "\n",
    "    score = model.evaluate(test_dataset[0], test_dataset[1], verbose=1)\n",
    "\n",
    "    print('loss : {}, acc : {}'.format(score[0], score[1]))\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    test_predictions = model.predict(test_dataset[0])\n",
    "    test_predictions = np.argmax(test_predictions, axis=1)\n",
    "\n",
    "    class_names = ['(0) Not recommended class', '(1) Recommended class']\n",
    "    report = classification_report(np.argmax(test_dataset[1], axis=1), test_predictions, target_names=class_names)\n",
    "    print(report)\n",
    "    conf_matrix = confusion_matrix(np.argmax(test_dataset[1], axis=1), test_predictions)\n",
    "    print(conf_matrix)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, annot_kws={'size': 16}, cmap='coolwarm', fmt='.2f')\n",
    "    plt.savefig('conf_matrix_recommendation.png', format='png', dpi=300)\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "\n",
    "    roc = roc_auc_score(y_score=test_predictions, y_true=np.argmax(test_dataset[1], 1))\n",
    "    print(roc)\n",
    "    from sklearn.metrics import auc\n",
    "    from sklearn.metrics import roc_curve\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(np.argmax(test_dataset[1], 1), test_predictions)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(roc_auc)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(fpr, tpr, lw=2, color='darkorange', label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc='lower right', fontsize=16)\n",
    "    plt.savefig('roc.png', format='png', dpi=300)\n",
    "    plt.show()\n",
    "    ## Sentiment Classification\n",
    "    labels = np.array(df['Sentiment'])\n",
    "    labels\n",
    "    labels = np.array([2 if label == 'Positive' else (1 if label == 'Neutral' else 0) for label in labels],\n",
    "                    np.int)\n",
    "    labels\n",
    "    positive_class = int(labels[labels == 2].shape[0])\n",
    "    neutral_class = int(labels[labels == 1].shape[0])\n",
    "    negative_class = int(labels[labels == 0].shape[0])\n",
    "\n",
    "    df = pd.DataFrame.from_dict({'positive': [positive_class], 'negative': [negative_class], 'neutral': [neutral_class]})\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    sns.set(font_scale=2)\n",
    "    sns.set_style('whitegrid')\n",
    "    ax = sns.barplot(data=df)\n",
    "    ax = ax.set_xlabel('Frequency Distribution of Sentiment Classes')\n",
    "    labels = to_categorical(labels)\n",
    "    train_dataset, validation_dataset, test_dataset = train_test_split(features=padded_features, labels=labels,\n",
    "                                                                    train_size=0.60, validation_size=0.50)\n",
    "    model = Sequential()\n",
    "    e = Embedding(vocabulary_size, max_length,\n",
    "                weights=[embedding_matrix], input_length=max_length, trainable=False)\n",
    "    model.add(e)\n",
    "    model.add(Bidirectional(LSTM(256), merge_mode='sum'))\n",
    "    model.add(Dropout(0.50))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    model.fit(train_dataset[0], train_dataset[1], epochs=32, batch_size=256, verbose=1,\n",
    "            validation_data=(validation_dataset[0], validation_dataset[1]))\n",
    "\n",
    "    score = model.evaluate(test_dataset[0], test_dataset[1], verbose=1)\n",
    "\n",
    "    print('loss : {}, acc : {}'.format(score[0], score[1]))\n",
    "    from sklearn.metrics import classification_report\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    test_predictions = model.predict(test_dataset[0])\n",
    "    test_predictions = np.argmax(test_predictions, axis=1)\n",
    "\n",
    "    class_names = ['(0) Negative class', '(1) Neutral class', '(2) Positive class']\n",
    "    report = classification_report(np.argmax(test_dataset[1], axis=1), test_predictions, target_names=class_names)\n",
    "    print(report)\n",
    "    conf_matrix = confusion_matrix(np.argmax(test_dataset[1], axis=1), test_predictions)\n",
    "    print(conf_matrix)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.savefig('conf_matrix_sentiment.png', format='png', dpi=300)\n",
    "    sns.heatmap(conf_matrix, annot=True, annot_kws={'size': 16}, cmap='coolwarm', fmt='.2f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "20e5f6fec62373a45a2ac35b699de802a60044274128ae40f21917c7aa140cfe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
